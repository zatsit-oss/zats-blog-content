---
slug: local-ai-assistant-in-vscode
title: Utiliser votre assistant IA en local (et sur VSCode) pour mieux respecter la planÃªte.
authors: [nfourre]
tags: [dev, web, green]
---

# Utiliser votre assistant IA en local (et sur VSCode) pour mieux respecter la planÃªte.

Vous rÃªvez d'un assistant intelligent comme GitHub Copilot, mais moins couteux en ressources machine, et gratuit ? Vous Ãªtes au bon endroit.

<!-- truncate -->

Dans cet article, nous allons vous guider, Ã©tape par Ã©tape, pour crÃ©er votre propre assistant IA local et le dÃ©ployer dans votre environnement de dÃ©veloppement (VSCode ici); Ã  lâ€™aide de [LM Studio](https://lmstudio.ai/) et de lâ€™extension [CodeGPT](https://docs.codegpt.co/docs/tutorial-basics/installation).

# Disclaimer

Vous le savez sans doute, l'utiliation d modÃ¨les d'IA nÃ©cessitent beaucoup de ressources machine : un processeur performant, une carte graphique puissante (GPU) et une grande quantitÃ© de RAM sont gÃ©nÃ©ralement recommandÃ©s (LM Studio permet de choisir lâ€™utilisation du CPU et/ou GPU, au passage).

# PrÃ©requis

Avant de commencer, assurez-vous dâ€™avoir NodeJS dâ€™installÃ© en version 18+ et Visual Studio Code (VSCode).

# Installation de LM Studio

ğŸ‘‰ Allez sur le site de [LM Studio] ([https://lmstudio.ai/download](https://lmstudio.ai/download)) et tÃ©lÃ©chargez la version adaptÃ©e Ã  votre matÃ©riel.

ğŸ‘‰ Une fois installÃ©e, il est temps de choisir le modÃ¨le le plus adaptÃ© Ã  votre besoin.
Nous vous proposons de tester le nouveau modÃ¨le de Mistral AI : [Codestral](https://mistral.ai/fr/news/codestral/), un modÃ¨le qui supporte plus de 80 langages de programmation. 
Il existe des modÃ¨les alternatifs pour des assistants de code comme [Code llama](https://codellama.dev/about) ou [StarCoder](https://github.com/bigcode-project/starcoder) si vous souhaitez comparer.

ğŸ‘‰ Ouvrez **LM Studio** et sÃ©lectionnez la loupe (dans le menu de gauche) afin de rechercher le modÃ¨le **Codestral** via la barre de recherche :

![LM Studio - Rechercher un modÃ¨le](lm-studio-1.webp)

![LM Studio - Ajouter un nouveau modÃ¨le](lm-studio-2.webp)

ğŸ‘‰ Une fois le modÃ¨le trouvÃ©, cliquez sur le bouton **Use in New Chat** pour tÃ©lÃ©charger le modÃ¨le.

*Patientez quelques instants, le temps de tÃ©lÃ©charger le modÃ¨le qui pÃ¨se un peu plus de 13 GB.*

ğŸ‘‰ Une fois tÃ©lÃ©chargÃ©, accÃ©dez au menu **Developer** (icÃ´ne **Terminal** sur la sidebar de gauche), puis sÃ©lectionnez le modÃ¨le Ã  charger dans la barre du haut :

![LM Studio - CrÃ©er un serveur local utilisant le modÃ¨le tÃ©lÃ©chargÃ©](lm-studio-3.webp)

ğŸ‘‰ Chargez le modÃ¨le **Codestral** que vous venez de tÃ©lÃ©charger. 
Patienter quelques instants, le modÃ¨le se charge en mÃ©moire. Une fois le chargement terminÃ©, il ne vous reste plus quâ€™Ã  cliquer sur **Start Server** pour commencer Ã  utiliser votre assistant.

![LM Studio - DÃ©marrer le serveur local](lm-studio-4.webp)

La configuration de **LM Studio** est maintenant terminÃ©e. Voyons comment effectuer son intÃ©gration dans **VS Code**.

# IntÃ©gration dans Visual Studio Code

ğŸ‘‰ La premiÃ¨re chose Ã  faire est de tÃ©lÃ©charger lâ€™extension [CodeGPT](https://docs.codegpt.co/docs/tutorial-basics/installation) qui permet, en sâ€™inscrivant sur leur site, dâ€™accÃ©der aux APIs de diffÃ©rentes IAs comme **ChatGPT** ou encore **ClaudeAI**.
Toutefois, ce nâ€™est pas ce que lâ€™on souhaite faire ici.

Nous cherchons Ã  utiliser l'infrastructure de nos machines, plutÃ´t que celles de serveurs distants, souvent alimentÃ©s avec une Ã©nergie Ã  la provenance inconnue et dans un datacenter sÃ»rement pas tout proche.

Une belle faÃ§on dâ€™Ãªtre un peu plus **green** et de rÃ©duire notre empreinte.

![VSCode - Ajouter l'extension CodeGPT](vscode-1.webp)

ğŸ‘‰ Une fois lâ€™extension installÃ©e, cliquez sur lâ€™icÃ´ne **ChatGPT** dans la *sidebar* de gauche, puis sÃ©lectionnez lâ€™assistant Ã  utiliser.

![VSCode - Configurer l'extension CodeGPT](vscode-2.webp)

ğŸ‘‰Ã€ la place de **CodeGPT Plus** (qui vous incite Ã  utiliser les API des GAFAM de lâ€™IA), choisissez **LM Studio** comme **Provider Local**.

![VSCode - Configurer le serveur local de LM Studio dans CodeGPT](vscode-3.webp)

Et voilÃ  ! Le tour est jouÃ©.

Vous avez Ã  prÃ©sent un assistant dâ€™aide Ã  la programmation, qui utilise directement vos ressources locales pour fonctionner.

Qui vous a dit qu'il n'Ã©tait pas possible de faire de l'IA frugale ?

# Utiliser votre nouvel assistant

Pour vÃ©rifier que lâ€™assistant fonctionne bien, nous allons lui demander de crÃ©er un composant *Product Card* en ReactJS avec une image, un nom du produit, une courte description, un prix et un bouton dâ€™ajout au panier, en interagissant directement avec la fenÃªtre de chat via le prompt suivant :

```
Create a product card component in react with product image, product name, short description, price and add to cart button
```

*Attention, les prompts sont Ã  rÃ©diger en anglais. Votre assistant ne fait pas encore la traduction automatique*

![VSCode - Utiliser votre assistant IA directement dans VSCode avec CodeGPT](vscode-4.webp)

Une fois le prompt envoyÃ©, lâ€™IA gÃ©nÃ©rative utilise le modÃ¨le **Codestral**  de votre **LM Studio** pour rÃ©pondre.

Si vous vÃ©rifiez la fenÃªtre de votre LM Studio, vous verrez bien le prompt et la rÃ©ponse dans les logs.

# Conclusion

Le code gÃ©nÃ©rÃ© par lâ€™assistant nâ€™est peut-Ãªtre pas encore au niveau de ce que peut produire [Bolt.new](http://Bolt.new) de StackBlitz ou [v0.dev](http://v0.dev) de Vercel, mais câ€™est dÃ©jÃ  une premiÃ¨re approche dâ€™utilisation de lâ€™IA beaucoup plus responsable (et Ã©conomique).

Cet assistant se rÃ©vÃ¨le toutefois plus efficace que ses alternatives, pour gÃ©nÃ©rer des tests unitaires Ã  partir de votre code ou pour optimiser ce dernier.

Maintenant, c'est Ã  vous de faire le reste : 
- explorer les possibilitÃ©s que vous offrent cet assistant 
- tester dâ€™autres modÃ¨les dâ€™IA (en local) plus adaptÃ©s Ã  vos besoins. 
- en parler autour de vous, pour favoriser l'adoption de cette technique.